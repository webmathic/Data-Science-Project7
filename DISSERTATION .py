# -*- coding: utf-8 -*-
"""DISSERTATION (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PPAA6eFQqCu3u2-oDzPyadobJhk_UNHo
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from scipy.stats import norm, boxcox
from scipy import stats
from collections import Counter
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import statsmodels.api as sm
import pickle as pk
import seaborn as sns

# Load the dataset into a pandas DataFrame
data = pd.read_csv('breast cancer data.csv')

# Display the first few rows of the dataset
data.head()

"""**Data Preprocessing**"""

# Count the number of missing values in each column
missing_values_count = data.isnull().sum()

# Print the number of missing values for each column
print("Number of missing values:")
print(missing_values_count)

data.describe()

# extract information from the data
data.info()

# Delete the patient ID column and also the delete the column with no information
data = data.drop(['id', 'Unnamed: 32'], axis=1)
data.head()

"""**feature engineering**
This code uses scikit-learn's LabelEncoder to convert categorical cancer diagnosis labels ('Malignant' and 'Benign') into numerical values (1 and 0), then displays the updated DataFrame.
"""

from sklearn.preprocessing import LabelEncoder

# Perform label encoding
label_encoder = LabelEncoder()
data['diagnosis'] = label_encoder.fit_transform(data['diagnosis'])

# Print the encoded DataFrame
print("Label encoded DataFrame:")
data.head()

"""This code uses scikit-learn's StandardScaler to standardize the numeric features in the DataFrame, excluding the 'diagnosis' column. It then prints the scaled DataFrame.





"""

from sklearn.preprocessing import StandardScaler

# Extract the numeric columns to scale
# Select numeric features excluding the 'diagnosis' column
numeric_columns = data.select_dtypes(include=np.number).columns.drop('diagnosis')


# Perform scaling
scaler = StandardScaler()
data[numeric_columns] = scaler.fit_transform(data[numeric_columns])

# Print the scaled DataFrame
print("Scaled DataFrame:")
data.head()

data['diagnosis'].unique()

"""This code separates features into three groups based on their column names (mean, s.e., and worst) using list comprehension. Then, it prints the extracted feature columns for each group."""

# Extract the mean features using list comprehension
mean_features = data[[col for col in data.columns if 'mean' in col]]

# Extract the s.e. (standard error) features using list comprehension
se_features = data[[col for col in data.columns if 'se' in col]]

# Extract the worst features using list comprehension
worst_features = data[[col for col in data.columns if 'worst' in col]]

# Print the extracted features
print("Mean Features:")
print(mean_features.columns)

print("\nS.E. Features:")
print(se_features.columns)

print("\nWorst Features:")
print(worst_features.columns)

"""This code defines a function generate_normal_distribution_plots that uses Seaborn to create normal distribution plots for selected features in the dataset. It arranges subplots based on the number of features and displays histograms with KDE for each feature and diagnosis class."""

def generate_normal_distribution_plots(data, features):
    # Set the figure size
    plt.figure(figsize=(20, 16))

    # Calculate the number of subplots based on the number of features
    num_plots = len(features)

    # Adjust the subplot layout dynamically based on the number of plots
    cols = 3
    rows = (num_plots - 1) // cols + 1

    # Iterate over each feature
    for i, feature in enumerate(features):
        # Create a normal distribution plot
        plt.subplot(rows, cols, i + 1)
        sns.histplot(data=data, x=feature, hue='diagnosis', kde=True)
        plt.title("Normal Distribution Plot - {}".format(feature), fontsize=20)
        plt.xlabel("Value", fontsize=20)
        plt.ylabel("Density", fontsize=20)

    # Adjust subplot spacing
    plt.tight_layout()

    # Show the plot
    plt.show()

# Call the generate_normal_distribution_plots() function for mean features
generate_normal_distribution_plots(data, mean_features.columns)

# Call the generate_normal_distribution_plots() function for standard error features
generate_normal_distribution_plots(data, se_features.columns)

# Call the generate_normal_distribution_plots() function for standard error features
generate_normal_distribution_plots(data, worst_features.columns)

"""This function generate_box_plots creates box plots for selected features in the dataset, grouped by diagnosis class. It dynamically arranges subplots and displays feature distributions for each diagnosis class using Seaborn's boxplot function.





"""

def generate_box_plots(data, features):
    # Set the figure size
    plt.figure(figsize=(20, 16))

    # Set the global font size for the plot
    plt.rc('font', size=15)


    # Calculate the number of subplots based on the number of features
    num_plots = len(features)

    # Adjust the subplot layout dynamically based on the number of plots
    cols = 3
    rows = (num_plots - 1) // cols + 1

    # Iterate over each feature
    for i, feature in enumerate(features):
        # Create a box plot
        plt.subplot(rows, cols, i + 1)
        sns.boxplot(data=data, x='diagnosis', y=feature)
        plt.title("Box Plot - {}".format(feature), fontsize=25)
        plt.xlabel("Diagnosis", fontsize=20)
        plt.ylabel("Value", fontsize=20)

    # Adjust subplot spacing
    plt.tight_layout()

    # Show the plot
    plt.show()

# Call the generate_box_plots() function for mean features
generate_box_plots(data, mean_features.columns)

# Call the generate_normal_distribution_plots() function for standard error features
generate_box_plots(data, se_features.columns)

# Call the generate_normal_distribution_plots() function for standard error features
generate_box_plots(data, worst_features.columns)

""" The function below utilizes Z-scores to identify and collect outlier values for each feature in a DataFrame. It's useful for detecting data points that significantly deviate from the mean."""

def detect_outliers_zscore(df, threshold=3):
    # Compute the Z-score for each feature
    zscores = (data - data.mean()) / data.std()

    # Create an empty DataFrame to store the outlier values
    outliers = pd.DataFrame(columns=data.columns)

    # Iterate over each feature
    for col in data.columns:
        # Find the outliers based on the Z-score threshold
        feature_outliers = zscores[col].abs() > threshold

        # Get the outlier values for the feature
        feature_values = data[col].loc[feature_outliers]

        # Store the outlier values in the DataFrame
        outliers[col] = feature_values

    return outliers
detect_outliers_zscore(data, threshold=3)

"""Detecting Outliers using interquartile range.

The code defines a function detect_outliers_iqr to find outliers in a DataFrame using the IQR method. It calculates quartiles and sets bounds for outlier detection. Outliers are identified if they fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR. The function returns a DataFrame with outlier values for each feature.
"""

def detect_outliers_iqr(df):
    # Calculate the IQR for each feature
    q1 = df.quantile(0.25)
    q3 = df.quantile(0.75)
    iqr = q3 - q1

    # Define the lower and upper bounds for outlier detection
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Create an empty DataFrame to store the outlier values
    outliers = pd.DataFrame(columns=df.columns)

    # Iterate over each feature
    for col in df.columns:
        # Find the outliers based on the lower and upper bounds
        feature_outliers = (df[col] < lower_bound[col]) | (df[col] > upper_bound[col])

        # Get the outlier values for the feature
        feature_values = df[col].loc[feature_outliers]

        # Store the outlier values in the DataFrame
        outliers[col] = feature_values

    return outliers

# Call the function to detect outliers using IQR method
detect_outliers_iqr(data)

"""The IQR method defines outliers based on the range between the first and third quartiles of the data. It is robust to outliers and resistant to extreme values, but it may not capture some extreme values that fall within the defined bounds.

On the other hand, the Z-score method standardizes the data using the mean and standard deviation. It identifies outliers as values with Z-scores greater than or less than a threshold. The Z-score method assumes a normal distribution and is sensitive to extreme values, potentially classifying slightly deviant values as outliers if they significantly affect the mean and standard deviation.

The output of IQR method is a superset of Z score method. Therefore, IQR is more robust to outliers and resistant to extreme value.

Remove outliers from the dataset using the interquartile range (IQR) method.
"""

def remove_outliers_iqr(df):
    # Calculate the IQR for each feature
    q1 = df.quantile(0.25)
    q3 = df.quantile(0.75)
    iqr = q3 - q1

    # Define the lower and upper bounds for outlier detection
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    # Iterate over each feature
    for col in df.columns:
        # Filter out the outliers based on the lower and upper bounds
        df = df.loc[(df[col] >= lower_bound[col]) & (df[col] <= upper_bound[col])]

    return df

# Call the function to remove outliers using IQR method
filtered_data = remove_outliers_iqr(data)

filtered_data.shape

"""Count the occurrences of each Diagnosis class"""

# Count the occurrences of each Diagnosis class
diagnosis_counts = filtered_data['diagnosis'].value_counts()

# Get the class labels and count values
labels = diagnosis_counts.index
counts = diagnosis_counts.values

# Set up the pie chart using seaborn
plt.figure(figsize=(8, 8))
sns.set(style='whitegrid')
plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=90, colors=sns.color_palette('pastel'))

# Set the title and aspect ratio
plt.title('Target Diagnosis')
plt.axis('equal')

# Show the plot
plt.show()

"""This code calculates the correlation matrix for a filtered dataset and displays it as a heatmap using Seaborn. The heatmap shows feature correlations, aiding in understanding relationships between features.


"""

# Calculate the correlation matrix
corr_matrix_ = filtered_data.corr()

# Set up the figure size
plt.figure(figsize=(20, 20))

# Create a heatmap of the correlation matrix
sns.heatmap(corr_matrix_, annot=True, cmap='viridis')

# Set the title and display the plot
plt.title('Correlation Matrix')
plt.show()

"""The code defines a function correlated_features to identify features with high and low correlations setting the  threshold to 0.9"""

def correlated_features(threshold):
    # Initialize empty sets for highly correlated and low correlated features
    highly_correlated_features = set()
    low_correlated_features = set()

    # Iterate over the correlation matrix
    for i in range(len(corr_matrix_.columns)):
        for j in range(i):
            correlation_value = corr_matrix_.iloc[i, j]

            if abs(correlation_value) > threshold:
                # If the correlation exceeds the threshold, add the feature names to the highly correlated set
                colname = corr_matrix_.columns[i]
                highly_correlated_features.add(colname)
            else:
                # If the correlation is below the threshold, add the feature names to the low correlated set
                colname = corr_matrix_.columns[i]
                low_correlated_features.add(colname)

    # Print the highly correlated features
    print("Highly correlated features:")
    for feature in highly_correlated_features:
        print(feature)
    print('')
    # Print the low correlated features
    print("Low correlated features:")
    for feature in corr_matrix_.columns:
        if feature not in highly_correlated_features:
            print(feature)


# Find highly correlated and low correlated features
threshold = 0.9  # Define the correlation threshold
correlated_features(threshold)

"""This plots visually presents relationships between these features and diagnoses, aiding in understanding their associations."""

high_corr_features = ['area_mean', 'perimeter_mean', 'radius_worst', 'area_worst', 'concave points_mean']

# Create a subset of the data with only the highly correlated features and the 'diagnosis' column
subset_data = filtered_data[high_corr_features + ['diagnosis']]

# Create a pair plot
sns.pairplot(subset_data, hue='diagnosis')

# Display the plot
plt.show()

high_corr_features = ['area_mean', 'perimeter_mean', 'radius_worst', 'area_worst', 'concave points_mean']

# Create a subset of the data with only the highly correlated features
subset_data = filtered_data[high_corr_features]

# Calculate the correlation matrix
correlation_matrix = subset_data.corr()

# Set up the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)

# Set plot title
plt.title('Correlation Matrix of Selected Features')

# Display the plot
plt.show()

uncorr_features = ['smoothness_mean', 'texture_mean', 'radius_mean', 'fractal_dimension_worst', 'symmetry_mean', 'symmetry_se']

# Create a subset of the data with only the  uncorrelated features and the 'diagnosis' column
subset_data = filtered_data[uncorr_features + ['diagnosis']]

# Create a pair plot
sns.pairplot(subset_data, hue='diagnosis')

# Display the plot
plt.show()

"""**FEATURE SELECTION**
Evaluate and compare different feature selection methods to identify the most informative features such as  using
1. The Filter method(checking for multicollinearity and variance inflation factor)
2. forward feature select method using SVM
3. forward feature select method using Random forest
4. Dimensional Reduction method(principal component analysis)

calculates and displays a heatmap of the correlation matrix among predictor variables, helping visualize their relationships.
"""

predictor_variables = filtered_data.drop('diagnosis', axis = 1)
# Calculate the correlation matrix
corr_matrix = predictor_variables.corr()

# Set up the figure size
plt.figure(figsize=(20, 20))

# Create a heatmap of the correlation matrix
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')

# Set the title and display the plot
plt.title('Correlation Matrix')
plt.show()

"""This code calculates the Variance Inflation Factor (VIF) for predictor variables to detect multicollinearity."""

from statsmodels.stats.outliers_influence import variance_inflation_factor

# Calculate the VIF for each predictor variable
vif = pd.DataFrame()
vif["Variable"] = predictor_variables.columns
vif["VIF"] = [variance_inflation_factor(predictor_variables.values, i) for i in range(predictor_variables.shape[1])]

# Display the VIF results
print(vif)

"""Feature selection 1 using VIF: This code removes variables with high VIF to decrease multicollinearity. It keeps variables with low VIF, shows their shape and initial rows.





"""

# Set the threshold for high VIF values
threshold =70

# Calculate the VIF for each predictor variable
vif = pd.DataFrame()
vif["Variable"] = predictor_variables.columns
vif["VIF"] = [variance_inflation_factor(predictor_variables.values, i) for i in range(predictor_variables.shape[1])]

# Select variables with low VIF values
low_vif_variables = vif[vif["VIF"] < threshold]["Variable"]

# Create a new DataFrame with variables having low VIF
predictor_variables_low_vif = predictor_variables[low_vif_variables]

# Display the variables with low VIF
features_1=predictor_variables_low_vif
print(features_1.shape)
features_1.head()

"""Feature selection 2 using forward selection with SVC:
This code uses Sequential Forward Feature Selector with a Support Vector Machine (SVC) model for feature selection. It identifies the most relevant features for classification and prints their names
"""

from sklearn.svm import SVC
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.model_selection import train_test_split

# Select predictor variables (features) and target variable
target_variable = filtered_data['diagnosis']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(predictor_variables, target_variable, test_size=0.2, random_state=42)

# Initialize the SVC model
svc_model = SVC()

# Forward selection using SequentialFeatureSelector
selector = SequentialFeatureSelector(svc_model, direction='forward')
selector.fit(X_train, y_train)

# Get the selected feature indices
selected_feature_indices = selector.get_support(indices=True)

# Get the selected feature names
selected_features_svc = X_train.columns[selected_feature_indices]

# Display the selected features
print("Selected Features:")
print(selected_features_svc)

# Create a DataFrame with the selected features with svc
predictor_variables_forward_selection_svc = predictor_variables.iloc[:, selected_feature_indices]

# Display the selected features DataFrame
print("Selected Features:")
features_2 =predictor_variables_forward_selection_svc
features_2

"""Feature selection 3: This code employs Sequential Feature Selector with a Random Forest Classifier to select significant features for classification. It then displays the names of the selected features."""

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SequentialFeatureSelector

# Select predictor variables (features) and target variable

target_variable = filtered_data['diagnosis']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(predictor_variables, target_variable, test_size=0.2, random_state=42)

# Initialize the Random Forest Classifier model
rf_model = RandomForestClassifier()

# Forward feature selection using SequentialFeatureSelector
selector = SequentialFeatureSelector(rf_model, direction='forward')
selector.fit(X_train, y_train)

# Get the selected feature indices
selected_feature_indices_rf = selector.get_support(indices=True)

# Get the selected feature names
selected_features_rf = X_train.columns[selected_feature_indices_rf]

# Create a DataFrame with the selected features
predictor_variables_forward_selection_rf = predictor_variables.iloc[:, selected_feature_indices_rf]

# Display the selected features
print("Selected Features:")
features_3 = predictor_variables_forward_selection_rf
features_3.columns

"""Feature selection 4 using PCA: This code applies Principal Component Analysis (PCA) to condense predictor variable dimensions and extract essential components."""

from sklearn.decomposition import PCA

# Standardize the predictor variables
scaler = StandardScaler()
predictor_variables_scaled = scaler.fit_transform(predictor_variables)

# Perform PCA
pca = PCA(n_components=4)  # Specify the number of components you want to keep
principal_components = pca.fit_transform(predictor_variables_scaled)

# Create a DataFrame with the principal components
principal_components_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3', 'PC4'])

# Display the explained variance ratio
print("Explained Variance Ratio:")
print(pca.explained_variance_ratio_)

# Display the principal components DataFrame
print("Principal Components:")
features_4=principal_components_df
features_4.head()

"""**Diagnosis Model - Support vector Classiffier for Feature selection 1, 2, 3 and 4**

This code trains an SVM (Support Vector Machine) model with a linear kernel and plots its decision boundary along with data points. The plot displays how the model separates 'Benign' and 'Malignant' cases based on the 'radius_mean' and 'texture_mean' features. The same process can be apply to any other selected features of choice.
"""

# Extract the features and target variable
X = predictor_variables[['radius_mean', 'texture_mean']]
y = filtered_data['diagnosis']

# Fit the SVC model
model = SVC(kernel='linear')
model.fit(X, y)

# Create a meshgrid of points to evaluate the model
x_min, x_max = X['radius_mean'].min() - 1, X['radius_mean'].max() + 1
y_min, y_max = X['texture_mean'].min() - 1, X['texture_mean'].max() + 1
h = 0.02  # step size in the mesh
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary
plt.figure(figsize=(10, 8))
plt.contourf(xx, yy, Z, cmap=plt.cm.Blues, alpha=0.3)

# Plot the data points
plt.scatter(X[y == 0]['radius_mean'], X[y == 0]['texture_mean'], color='b', label='Benign')
plt.scatter(X[y == 1]['radius_mean'], X[y == 1]['texture_mean'], color='r', label='Malignant')

# Set plot labels and title
plt.xlabel('Radius Mean')
plt.ylabel('Texture Mean')
plt.title('SVC Decision Boundary')

# Show legend
plt.legend()

# Show the plot
plt.show()

"""we can also calculates and prints the best hyperplane and margin."""

from sklearn.svm import SVC

def plot_svc_decision_boundary(X, y):
    # Convert X to a NumPy array
    X = X.values

    # Fit the SVC model
    model = SVC(kernel='linear')
    model.fit(X, y)

    # Create a meshgrid of points to evaluate the model
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    h = 0.02  # step size in the mesh
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    # Set plot limits and figure size
    plt.figure(figsize=(12, 10))
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)

    # Plot the decision boundary, hyperplane, margin, and support vectors
    plt.contourf(xx, yy, Z, cmap=plt.cm.Blues, alpha=0.3)

    # Plot the data points
    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='Class 0', s=80)
    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='Class 1', s=80)

    # Get the separating hyperplane
    w = model.coef_[0]
    a = -w[0] / w[1]
    xx_hyperplane = np.linspace(x_min, x_max)
    yy_hyperplane = a * xx_hyperplane - (model.intercept_[0]) / w[1]

    # Plot the hyperplane
    plt.plot(xx_hyperplane, yy_hyperplane, 'k-', label='Hyperplane')

    # Get the parallel support vectors
    margin = 1 / np.sqrt(np.sum(model.coef_ ** 2))
    yy_margin_top = yy_hyperplane + a * margin
    yy_margin_bottom = yy_hyperplane - a * margin

    # Plot the margin
    plt.plot(xx_hyperplane, yy_margin_top, 'k--', label='Margin')
    plt.plot(xx_hyperplane, yy_margin_bottom, 'k--')

    # Plot support vectors
    support_vectors = model.support_vectors_
    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], color='lime', label='Support Vectors', s=120, edgecolors='black')

    # Set plot labels and title
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.title('SVC Decision Boundary')

    # Show legend
    plt.legend()

    # Show the plot
    plt.show()


# Extract the features and target variable
X = predictor_variables[['radius_mean', 'texture_mean']]
y = filtered_data['diagnosis']
plot_svc_decision_boundary(X, y)

# Get the best hyperplane and margin
model = SVC(kernel='linear')
model.fit(X, y)
best_hyperplane = model.coef_[0]
margin = 1 / np.sqrt(np.sum(best_hyperplane ** 2))
print("Best Hyperplane:", best_hyperplane)
print("Margin:", margin)

"""This code defines a function train_and_evaluate that trains and evaluates an SVM model using grid search cross-validation. It calculates and prints metrics such as best hyperparameters, cross-validation score, accuracy, precision, recall, F1-score, and confusion matrix. The function takes predictor variables X, target variable y, the model to evaluate, and a parameter grid for grid search as inputs. It performs grid search, trains the best model, predicts on the test set, and computes evaluation metrics."""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def train_and_evaluate(X, y, model, param_grid):
    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create a grid search cross-validation object
    grid_search = GridSearchCV(model, param_grid, cv=10)

    # Fit the grid search to the training data
    grid_search.fit(X_train, y_train)

    # Get the best hyperparameters and CV score
    best_params_svm = grid_search.best_params_
    best_score_svm = grid_search.best_score_

    # Get the best model based on the grid search
    best_model_svm = grid_search.best_estimator_

    # Train the best model on the full training set
    best_model_svm.fit(X_train, y_train)

    # Predict on the test set using the best model
    y_pred_svm = best_model_svm.predict(X_test)

    # Compute evaluation metrics
    accuracy_svm = accuracy_score(y_test, y_pred_svm)
    precision_svm = precision_score(y_test, y_pred_svm)
    recall_svm = recall_score(y_test, y_pred_svm)
    f1_svm = f1_score(y_test, y_pred_svm)
    cm_svm = confusion_matrix(y_test, y_pred_svm)

    # Print the evaluation metrics
    print("Best Hyperparameters:", best_params_svm)
    print("Best CV Score:", best_score_svm)
    print("Accuracy:", accuracy_svm)
    print("Precision:", precision_svm)
    print("Recall:", recall_svm)
    print("F1-Score:", f1_svm)
    print("Confusion Matrix:\n", cm_svm)

"""SVC for features 1"""

X=features_1
y= filtered_data['diagnosis']
model = SVC()
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['rbf', 'linear'],
    'gamma': [0.1, 1, 10]
}
train_and_evaluate(X, y, model, param_grid)

"""SVC for features 2"""

X=features_2
y= filtered_data['diagnosis']
model = SVC()
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['rbf', 'linear'],
    'gamma': [0.1, 1, 10]
}
train_and_evaluate(X, y, model, param_grid)

"""SVC for features 3"""

X=features_3
y= filtered_data['diagnosis']
model = SVC()
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['rbf', 'linear'],
    'gamma': [0.1, 1, 10]
}
train_and_evaluate(X, y, model, param_grid)

"""SVC for features 4"""

X=features_4
y= filtered_data['diagnosis']
model = SVC()
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['rbf', 'linear'],
    'gamma': [0.1, 1, 10]
}
train_and_evaluate(X, y, model, param_grid)

"""**Diagnosis Model - Random forest classifier Classiffier for all features**

the code trains a Random Forest classifier and visualizes its first decision tree. We can visualize other decision trees by increasing the estimate index.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import plot_tree


X = predictor_variables
target_names = {0: 'Benign', 1: 'Malignant'}
y1 = filtered_data['diagnosis'].map(target_names)



# Train a Random Forest classifier
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest.fit(X, y1)

# Plot the first decision tree in the Random Forest
plt.figure(figsize=(20, 18))
plot_tree(random_forest.estimators_[0], feature_names=X.columns, class_names=target_names, filled=True)
plt.show()

"""performing thesame precedure we define a function train_and_evaluate that trains and evaluates a Random Forest classifier using grid search. It calculates and prints metrics including best hyperparameters, cross-validation score, accuracy, precision, recall, F1-score, and confusion matrix. The function takes predictor variables X, target variable y, and a parameter grid, performing grid search, training, prediction, and metric calculation.





"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def train_and_evaluate(X, y, param_grid):
    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create a Random Forest classifier
    rf = RandomForestClassifier()

    # Create a grid search cross-validation object
    grid_search = GridSearchCV(rf, param_grid, cv=5)

    # Fit the grid search to the training data
    grid_search.fit(X_train, y_train)

    # Get the best hyperparameters and CV score
    best_params_rf = grid_search.best_params_
    best_score_rf = grid_search.best_score_

    # Get the best model based on the grid search
    best_model_rf = grid_search.best_estimator_

    # Train the best model on the full training set
    best_model_rf.fit(X_train, y_train)

    # Predict on the test set using the best model
    y_pred_rf = best_model_rf.predict(X_test)

    # Compute evaluation metrics
    accuracy_rf = accuracy_score(y_test, y_pred_rf)
    precision_rf = precision_score(y_test, y_pred_rf)
    recall_rf = recall_score(y_test, y_pred_rf)
    f1_rf = f1_score(y_test, y_pred_rf)
    cm_rf = confusion_matrix(y_test, y_pred_rf)

    # Print the evaluation metrics
    print("Best Hyperparameters:", best_params_rf)
    print("Best CV Score:", best_score_rf)
    print("Accuracy:", accuracy_rf)
    print("Precision:", precision_rf)
    print("Recall:", recall_rf)
    print("F1-Score:", f1_rf)
    print("Confusion Matrix:\n", cm_rf)

"""Training and testing features 1 with Random Forest"""

X = features_1
y = filtered_data['diagnosis']

# Define the parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

# Call the train_and_evaluate function
train_and_evaluate(X, y, param_grid)

"""Training and testing features 2 with Random Forest"""

X = features_2
y = filtered_data['diagnosis']

# Define the parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

# Call the train_and_evaluate function
train_and_evaluate(X, y, param_grid)

"""Training and testing features 3 with Random Forest"""

X = features_3
y = filtered_data['diagnosis']

# Define the parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

# Call the train_and_evaluate function
train_and_evaluate(X, y, param_grid)

"""Training and testing features 4 with Random Forest"""

X = features_4
y = filtered_data['diagnosis']

# Define the parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

# Call the train_and_evaluate function
train_and_evaluate(X, y, param_grid)

"""A Function to get the most important feature for each model"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

def get_feature_importance(X, y):
    # Train a Random Forest classifier
    random_forest = RandomForestClassifier(n_estimators=100, random_state=42)
    random_forest.fit(X, y)

    # Get feature importances
    importances = random_forest.feature_importances_
    indices = np.argsort(importances)[::-1]
    feature_names = X.columns

    # Create a sorted dataframe of feature importances
    importance_df = pd.DataFrame({'Feature': feature_names[indices], 'Importance': importances[indices]})

    # Plot feature importances using seaborn
    plt.figure(figsize=(12, 8))
    sns.barplot(x='Feature', y='Importance', data=importance_df, palette='viridis')
    plt.title("Feature Importance", fontsize=16)
    plt.xlabel("Feature", fontsize=14)
    plt.ylabel("Importance", fontsize=14)
    plt.xticks(rotation=90, fontsize=12)
    plt.yticks(fontsize=12)

    # Show plot
    plt.tight_layout()
    plt.show()

    # Return feature importances
    return importances, feature_names[indices]

""" Importance features for features 1"""

# Features importance for model 1:
X = features_1
y = filtered_data['diagnosis']
importances, feature_names = get_feature_importance(X, y)

""" Importance features for features 2"""

# Features importance for model 2:
X = features_2
y = filtered_data['diagnosis']
importances, feature_names = get_feature_importance(X, y)

""" Importance features for model 3"""

# Features importance for model 3:
X = features_3
y = filtered_data['diagnosis']
importances, feature_names = get_feature_importance(X, y)

""" Importance features for features 4"""

# Features importance for model 4:
X = features_4
y = filtered_data['diagnosis']
importances, feature_names = get_feature_importance(X, y)

"""The function uses grid search to train and evaluate a Logistic Regression classifier."""

from sklearn.linear_model import LogisticRegression

def train_and_evaluate(X, y, param_grid):
    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Create a logistic regression classifier
    lr = LogisticRegression()

    # Create a grid search cross-validation object
    grid_search = GridSearchCV(lr, param_grid, cv=10)

    # Fit the grid search to the training data
    grid_search.fit(X_train, y_train)

    # Get the best hyperparameters and CV score
    best_params_lr = grid_search.best_params_
    best_score_lr = grid_search.best_score_

    # Get the best model based on the grid search
    best_model_lr = grid_search.best_estimator_

    # Train the best model on the full training set
    best_model_lr.fit(X_train, y_train)

    # Predict on the test set using the best model
    y_pred_lr = best_model_lr.predict(X_test)

    # Compute evaluation metrics
    accuracy_lr = accuracy_score(y_test, y_pred_lr)
    precision_lr = precision_score(y_test, y_pred_lr)
    recall_lr = recall_score(y_test, y_pred_lr)
    f1_lr = f1_score(y_test, y_pred_lr)
    cm_lr = confusion_matrix(y_test, y_pred_lr)

    # Print the evaluation metrics
    print("Best Hyperparameters:", best_params_lr)
    print("Best CV Score:", best_score_lr)
    print("Accuracy:", accuracy_lr)
    print("Precision:", precision_lr)
    print("Recall:", recall_lr)
    print("F1-Score:", f1_lr)
    print("Confusion Matrix:\n", cm_lr)

"""Training and testing feature 1 with Logistic Regression"""

X = features_1
y = filtered_data['diagnosis']

# Define the parameter grid for grid search
param_grid = {
    'C': [0.1, 1, 10],
    'penalty': ['l1', 'l2']
}

# Call the train_and_evaluate function
train_and_evaluate(X, y, param_grid)

"""Training and testing feature 2 with Logistic Regression"""

X = features_2
y = filtered_data['diagnosis']

# Define the parameter grid for grid search
param_grid = {
    'C': [0.1, 1, 10],
    'penalty': ['l1', 'l2']
}

# Call the train_and_evaluate function
train_and_evaluate(X, y, param_grid)

"""Training and testing features 3 with Logistic Regression"""

X = features_3
y = filtered_data['diagnosis']

# Define the parameter grid for grid search
param_grid = {
    'C': [0.1, 1, 10],
    'penalty': ['l1', 'l2']
}

# Call the train_and_evaluate function
train_and_evaluate(X, y, param_grid)

"""Training and testing features 4 with Logistic Regression"""

X = features_4
y = filtered_data['diagnosis']

# Define the parameter grid for grid search
param_grid = {
    'C': [0.1, 1, 10],
    'penalty': ['l1', 'l2']
}

# Call the train_and_evaluate function
train_and_evaluate(X, y, param_grid)

"""**Bar chart graphical representation for models performance**"""

import pandas as pd
import matplotlib.pyplot as plt

def plot_model_performance(models_metrics):
    # Create a DataFrame from the models_metrics dictionary
    df = pd.DataFrame(data=models_metrics)
    df.rename(index={0: 'Accuracy', 1: 'Precision', 2: 'Recall', 3: 'F1-Score'}, inplace=True)

    # Plot the bar chart
    ax = df.plot(kind='bar', figsize=(18, 15), ylim=(0.6, 1.1),
                 color=['orange', 'green', 'blue'],
                 rot=0, title='Model Performance',
                 edgecolor='grey', alpha=0.5)

    # Annotate the bars with their values
    for p in ax.patches:
        ax.annotate(f'{p.get_height():.2f}', (p.get_x() * 1.01, p.get_height() * 1.005))

    # Show the plot
    plt.show()

"""Bar chart representation of Evaluation metrics for features 1"""

features1_metrics = {
    'SVM': [0.95, 1.0, 0.84, 0.91],
    'RF': [0.91, 0.95, 0.80, 0.87],
    'LOG': [0.94, 0.95, 0.84, 0.89]
}

plot_model_performance(features1_metrics)

"""Bar chart representation of Evaluation metrics for features 2"""

features2_metrics = {
    'SVM': [0.96, 1.0, 0.88, 0.94],
    'RF': [0.91, 1.0, 0.80, 0.89],
    'LOG': [0.96, 1.0, 0.88, 0.94]
}

plot_model_performance(features2_metrics)

"""Bar chart representation of Evaluation metrics for features 3"""

features3_metrics = {
    'SVM': [0.95, 0.96, 0.88, 0.92],
    'RF': [0.90, 1.0, 0.76, 0.86],
    'LOG': [0.96, 1.0, 0.88, 0.94]
}

plot_model_performance(features3_metrics)

"""Bar chart representation of Evaluation metrics for features 4"""

features4_metrics = {
    'SVM': [0.88, 0.82, 0.76, 0.79],
    'RF': [0.91, 0.91, 0.80, 0.85],
    'LOG': [0.90, 0.87, 0.80, 0.83]
}

plot_model_performance(features4_metrics)

"""This code shows  a function plot_roc_auc to plot ROC curves and AUC scores for different models and feature selections. It demonstrates this function with SVM, Random Forest, and Logistic Regression models. ROC curves and AUC values are shown for each combination, along with a random guessing curve, creating a comparative plot.





"""

from sklearn.metrics import roc_curve, roc_auc_score

def plot_roc_auc(model, predictor_variables, target_variable, label):
    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(predictor_variables, target_variable, test_size=0.2, random_state=42)

    # Fit the model to the training data
    model.fit(X_train, y_train)

    # Calculate predicted probabilities
    if hasattr(model, 'predict_proba'):
        y_pred_proba = model.predict_proba(X_test)[:, 1]
    else:
        y_pred_proba = model.decision_function(X_test)

    # Calculate the ROC curve
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)

    # Plot the ROC curve
    plt.plot(fpr, tpr, lw=2, label=f'{label} (AUC = {auc:.2f})')

# Example usage with different ML models and predictor variables
svm_model = SVC(probability=True)
rf_model = RandomForestClassifier()
lr_model = LogisticRegression()

# Use all feature selection methods
predictor_variables1 = features_1
predictor_variables2 = features_2
predictor_variables3 = features_3
target_variable = filtered_data['diagnosis']  # Replace 'filtered_data' and 'diagnosis' with the appropriate dataset and target variable

# Create a single plot for all feature selection methods
plt.figure(figsize=(10, 6))

# Plot ROC curve and AUC for each ML model using each feature selection method
plot_roc_auc(svm_model, predictor_variables1, target_variable, 'SVM - Feature Selection 1')
plot_roc_auc(rf_model, predictor_variables1, target_variable, 'Random Forest - Feature Selection 1')
plot_roc_auc(lr_model, predictor_variables1, target_variable, 'Logistic Regression - Feature Selection 1')

plot_roc_auc(svm_model, predictor_variables2, target_variable, 'SVM - Feature Selection 2')
plot_roc_auc(rf_model, predictor_variables2, target_variable, 'Random Forest - Feature Selection 2')
plot_roc_auc(lr_model, predictor_variables2, target_variable, 'Logistic Regression - Feature Selection 2')

plot_roc_auc(svm_model, predictor_variables3, target_variable, 'SVM - Feature Selection 3')
plot_roc_auc(rf_model, predictor_variables3, target_variable, 'Random Forest - Feature Selection 3')
plot_roc_auc(lr_model, predictor_variables3, target_variable, 'Logistic Regression - Feature Selection 3')

# Plot the random guessing curve
plt.plot([0, 1], [0, 1], 'k--')

plt.xlim([-0.1, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('Receiver Operating Characteristic (ROC) Curves and AUC for Different Feature Selection Methods')
plt.legend(loc='lower right')
plt.grid(True)  # Add gridlines to the plot
plt.show()

"""This code presents evaluation metrics for three different models: SVM, Random Forest, and Logistic Regression. The metrics include accuracy, precision, recall, and F1-score, each presented for different features selected datasets."""

# Evaluation metrics for SVM model
svm_eval_metrics = [
    [0.95, 1.00, 0.84, 0.91],
    [0.96, 1.00, 0.88, 0.94],
    [0.95, 1.00, 0.88, 0.94],
    [0.88, 0.83, 0.76, 0.79]
]

# Evaluation metrics for Random Forest model
rf_eval_metrics = [
    [0.91, 1.00, 0.80, 0.89],
    [0.91, 1.00, 0.80, 0.89],
    [0.90, 0.91, 0.80, 0.85],
    [0.91, 0.91, 0.80, 0.85]
]

# Evaluation metrics for Logistic Regression model
lr_eval_metrics = [
    [0.94, 0.95, 0.84, 0.89],
    [0.96, 1.00, 0.88, 0.94],
    [0.96, 1.00, 0.84, 0.91],
    [0.90, 0.87, 0.87, 0.83]
]

"""One-way ANOVA test on the evaluation metrics of three different models (SVM, Random Forest, Logistic Regression). It calculates p-values for the ANOVA test and identifies the best model based on the smallest p-value. Finally, it prints the ANOVA results and the name of the best-performing model."""

import numpy as np
from scipy.stats import f_oneway

# Perform one-way ANOVA test on evaluation metrics
anova_results = f_oneway(svm_eval_metrics, rf_eval_metrics, lr_eval_metrics)

# Get the p-values
p_values = anova_results.pvalue

# Find the index of the best model (smallest p-value)
best_model_idx = np.argmin(p_values)

models = ['Support Vector Machine', 'Random Forest Classifier', 'Logistic Regression']
best_model = models[best_model_idx]

# Print the ANOVA results and the best model
print("ANOVA Results:")
print("p-values:", p_values)
print("Best Model:", best_model)

"""Akaike Information Criterion (AIC) for each of three models (SVM, Random Forest, Logistic Regression) based on their predicted values and the true target values. It uses the calculated AIC values to compare the models in terms of their goodness of fit and complexity. The lower the AIC, the better the model fits the data while accounting for the number of parameters. The code then prints out the AIC values for each model. SVM having the lowest AIC value is the best model for diagnosing breast cancer."""

# Calculate AIC for each model
def calculate_aic(y_test, y_pred, k):
    n = len(y_test)  # Number of observations
    residuals = y_test - y_pred  # Residuals
    mse = np.sum(residuals**2) / n  # Mean squared error
    aic = n * np.log(mse) + 2 * k  # AIC value
    return aic

# Assuming you have the target variable for each model
# For example, y_test_svm, y_test_rf, y_test_lr

# SVM Model
y_test_svm = np.array([1, 0, 1, 0])  # Example true target values for SVM
y_pred_svm = np.mean(svm_eval_metrics, axis=1)  # Using average as predicted values
k_svm = len(svm_eval_metrics[0]) + 1  # Number of parameters (features + intercept)
aic_svm = calculate_aic(y_test_svm, y_pred_svm, k_svm)

# Random Forest Model
y_test_rf = np.array([1, 0, 1, 0])  # Example true target values for Random Forest
y_pred_rf = np.mean(rf_eval_metrics, axis=1)  # Using average as predicted values
k_rf = len(rf_eval_metrics[0]) + 1  # Number of parameters (features + intercept)
aic_rf = calculate_aic(y_test_rf, y_pred_rf, k_rf)

# Logistic Regression Model
y_test_lr = np.array([1, 0, 1, 0])  # Example true target values for Logistic Regression
y_pred_lr = np.mean(lr_eval_metrics, axis=1)  # Using average as predicted values
k_lr = len(lr_eval_metrics[0]) + 1  # Number of parameters (features + intercept)
aic_lr = calculate_aic(y_test_lr, y_pred_lr, k_lr)

# Print AIC for each model
print("AIC for Support Vector Machine:", aic_svm)
print("AIC for Random Forest:", aic_rf)
print("AIC for Logistic Regression:", aic_lr)

# Find the best model based on the lowest AIC value
aic_values = [aic_svm, aic_rf, aic_lr]
best_model_index = np.argmin(aic_values)
best_model_names = ["Support Vector Machine", "Random Forest", "Logistic Regression"]

print("The best model is:", best_model_names[best_model_index])

"""I defines lists of F1-scores for three different models: SVM, Random Forest, and Logistic Regression. These F1-scores are used for performing statistical analysis for the one-way ANOVA test to compare the performance of these models."""

# F1-Scores for SVM model
svm_f1_scores = [0.91, 0.94, 0.94, 0.79]

# F1-Scores for Random Forest model
rf_f1_scores = [0.89, 0.89, 0.85, 0.85]

# F1-Scores for Logistic Regression model
lr_f1_scores = [0.89, 0.94, 0.91, 0.83]

"""The one way ANOVA test is used to assess whether there are statistically significant differences among the F1-scores of these models. The code then prints out the one way ANOVA results, including the p-value obtained from the test."""

from scipy.stats import f_oneway

# Perform one-way ANOVA test on F1-Scores
anova_result = f_oneway(svm_f1_scores, rf_f1_scores, lr_f1_scores)

# Print the ANOVA results
print("ANOVA Results:")
print("p-value:", anova_result.pvalue)

"""The ANOVA results will give us a p-value of 0.75 which indicates that there  are no significant differences in the models results under 95% confidence interval(alpha= 0.05), since the p-value is greater than the  significance level,  """